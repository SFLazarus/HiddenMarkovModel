{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2:  Hidden Markov Model \n",
    "\n",
    "## Probabilistic states and transitions\n",
    "1. Set up a new git repository in your GitHub account\n",
    "2. Pick a text corpus dataset such as\n",
    "https://www.kaggle.com/kingburrito666/shakespeare-plays\n",
    "or from https://github.com/niderhoff/nlp-datasets\n",
    "3. Choose a programming language (Python, C/C++, Java)\n",
    "4. Formulate ideas on how machine learning can be used to learn word correlations and distributions within the dataset\n",
    "5. Build a Hidden Markov Model to be able to programmatically\n",
    "1. Generate new text from the text corpus\n",
    "2. Perform text prediction given a sequence of words\n",
    "6. Document your process and results\n",
    "7. Commit your source code, documentation and other supporting files to the git repository in GitHub GRAPHICAL MODELS\n",
    "\n",
    "## Step-1 Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2 Importing NEWS headlines data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>headline_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>765515</td>\n",
       "      <td>five steps to saving your brand during a crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2758743</td>\n",
       "      <td>supernatural season 9 finale photos sam dean a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2186988</td>\n",
       "      <td>minnesota dog destroyed after two year fight t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7232</td>\n",
       "      <td>rachel does it again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253863</td>\n",
       "      <td>average joes pub is above average</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                    headline_tokens\n",
       "0   765515    five steps to saving your brand during a crisis\n",
       "1  2758743  supernatural season 9 finale photos sam dean a...\n",
       "2  2186988  minnesota dog destroyed after two year fight t...\n",
       "3     7232                               rachel does it again\n",
       "4   253863                  average joes pub is above average"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"headline_tokens\"]\n",
    "data=pd.read_csv(\"../data/examiner-date-tokens.csv\")[columns].sample(100000).reset_index()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/therohk/examine-the-examiner?select=examiner-date-tokens.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3 Pre-processing data\n",
    "\n",
    "### Building stop words library \n",
    "- which are a collection of english that which are too common and would affect our model as these words are mostly repeated in sentences, they have higher probabilities and lesser meaning while we generate new text, also there are risks of infinite looping during text generation such as in my case I faced this: `King of King of King of King of` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords=[\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing each line of headlines data\n",
    "- We need to remove all the special characters\n",
    "- Convert to lower case\n",
    "- then we remove the stop words from headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listOfWordsInLines=[]\n",
    "for headline in data['headline_tokens']:\n",
    "    processedHeadline=re.sub('[^A-Za-z0-9 ]+', '', headline).lower()\n",
    "    processedHeadlineList=processedHeadline.split()  \n",
    "    processedHeadlineList = [word for word in processedHeadlineList if word not in stopWords]\n",
    "#     print(processedHeadlineList)\n",
    "    listOfWordsInLines.append(processedHeadlineList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['concrete', 'blonde', 'performs', 'bloodletting', 'album', 'live', 'austin', 'tx', 'june', '19th']\n"
     ]
    }
   ],
   "source": [
    "print(listOfWordsInLines[7000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4 Calculating Word Frequencies\n",
    "- we can use this function to calculate immediately next word's frequencies or also the second next following word's frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordfrequencies(whichSuccessor=1):\n",
    "    nextWordFrequencies = {} \n",
    "    for i in range(len(listOfWordsInLines)):\n",
    "        listOfWordsInCurrentLine = listOfWordsInLines[i]\n",
    "        for wordIndex in range(len(listOfWordsInCurrentLine)-whichSuccessor):\n",
    "            currentWord=listOfWordsInCurrentLine[wordIndex]\n",
    "            nextWord=listOfWordsInCurrentLine[wordIndex+whichSuccessor] \n",
    "            if currentWord not in nextWordFrequencies:\n",
    "                nextWordFrequencies[currentWord]={nextWord:1}\n",
    "            else:\n",
    "                if nextWord not in nextWordFrequencies[currentWord].keys():\n",
    "                    nextWordFrequencies[currentWord][nextWord] = 1\n",
    "                else:\n",
    "                    nextWordFrequencies[currentWord][nextWord] += 1\n",
    "    return nextWordFrequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5 Calculating Probabilities\n",
    "- we can use this function to calculate immediately next word's probabilities or also the second next following word's probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextWordProbabilities(wordFrequencies):\n",
    "    for currentword in wordFrequencies:\n",
    "        totalFrequenciesForCurrentWord=0\n",
    "        for nextWord in wordFrequencies[currentword]:\n",
    "            totalFrequenciesForCurrentWord+=wordFrequencies[currentword][nextWord]\n",
    "        for nextWord in wordFrequencies[currentword]:\n",
    "            currentNextWordFrequency=wordFrequencies[currentword][nextWord]\n",
    "            wordFrequencies[currentword][nextWord]=currentNextWordFrequency/totalFrequenciesForCurrentWord\n",
    "    return wordFrequencies\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6 Building probability distributions\n",
    "-Here by using above methods, we need next word probabilities and second next word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "allNextWordProbabilities=nextWordProbabilities(wordfrequencies(1))\n",
    "\n",
    "allThirdWordProbabilities=nextWordProbabilities(wordfrequencies(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7 Complete the sentence Method\n",
    "- It takes a line as input with atleast two words and by using the last word and penultimate word we predict the following word using next word probabilities and second next word probabilities respectively. This continues until we reach the no of words to be predicted limit, which is passed as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completeTheSentence(inputLine, secondWordProbabilities={},thirdWordProbabilities={}, maximumWords=7):\n",
    "    prediction=inputLine+\" \"\n",
    "    inputLineList=str(re.sub('[^A-Za-z0-9 ]+', '', inputLine)).lower().split()\n",
    "#     print(inputLineList[-1])\n",
    "    lastWord=inputLineList[-1]\n",
    "    penultimateWord=inputLineList[-2]\n",
    "    predictionProbabilities={}\n",
    "    for i in range(maximumWords):\n",
    "        lastWordProbabilities=secondWordProbabilities[lastWord]\n",
    "        penultimateWordProbabilities=thirdWordProbabilities[penultimateWord]\n",
    "\n",
    "        \n",
    "        for nextPossibleWord in lastWordProbabilities.keys():\n",
    "            if nextPossibleWord in penultimateWordProbabilities.keys():\n",
    "    #             print(nextPossibleWord)\n",
    "    #             print(lastWordProbabilities[nextPossibleWord])\n",
    "    #             print(penultimateWordProbabilities[nextPossibleWord])\n",
    "                predictionProbabilities[nextPossibleWord]=lastWordProbabilities[nextPossibleWord]*penultimateWordProbabilities[nextPossibleWord]\n",
    "\n",
    "        if(len(predictionProbabilities)==0):\n",
    "            nextPossibleWord=random.choices(list(allNextWordProbabilities[lastWord].keys()))\n",
    "            break\n",
    "        predictNext=max(predictionProbabilities,key=predictionProbabilities.get)\n",
    "#         print(predictNext)\n",
    "#         print(predictionProbabilities)\n",
    "        del predictionProbabilities[predictNext]\n",
    "        penultimateWord=lastWord\n",
    "        lastWord=predictNext\n",
    "        prediction=\"\".join((prediction,predictNext,\" \"))\n",
    "    return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Auto Sentence Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 free food fun wine events weekend week april '"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeTheSentence(\"10 free\", allNextWordProbabilities,allThirdWordProbabilities )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'top pop music awards 2011 2010 season 2 episode 2 recap spoilers '"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeTheSentence(\"top pop\", allNextWordProbabilities,allThirdWordProbabilities,10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We could generate some good predictions although some parts of it doesn't make sense but still we have a good sentence formation\n",
    "\n",
    "## Step-8 Generate New Headlines method\n",
    "- If we specify how many headlines we need along with how many how many words, it randomly generates new headlines for us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_headlines(maximumWords=15,maximumLines=5):\n",
    "    for i in range(maximumLines):\n",
    "        firstWord=random.choices(list(allNextWordProbabilities.keys()))\n",
    "        secondWord=random.choices(list(allNextWordProbabilities[firstWord[0]].keys()))\n",
    "        line= firstWord[0]+\" \"+secondWord[0]\n",
    "    #     print(line)\n",
    "        print(completeTheSentence(line, allNextWordProbabilities,allThirdWordProbabilities,maximumWords ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Generating Random Headlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incomplete untruthful shares trade secrets new jersey city book \n",
      "62nd annual national primetime day park show weekend 2 \n",
      "disagreements says report may 1 2010 part 2 3 \n",
      "wwii pilots wins first national chicago round 1 2 \n"
     ]
    }
   ],
   "source": [
    "generate_new_headlines(7,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, we have randomly generated headlines with good sentence structures and with few errors in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
